# -*- coding: utf-8 -*-
"""Copy of HIC_using_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oe_vPTt-dgDC97Oq0GDkwPABVWA34NMN
"""

from __future__ import print_function
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.utils import np_utils
from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, Activation, GlobalMaxPool1D
from keras.layers import Dropout, Input, InputLayer, GlobalMaxPool2D, Conv1D, MaxPool1D, MaxPool2D
from keras.models import Model
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
from keras.backend import concatenate

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score



from operator import truediv
from plotly.offline import init_notebook_mode
from tqdm import tqdm

import scipy.io as sio

## Read the data.
X = loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']
y = loadmat('Indian_pines_gt.mat')['indian_pines_gt']

#Plotting ground truth
plt.figure(figsize=(8,6))
plt.imshow(y, cmap='nipy_spectral')
plt.axis('off')
plt.show()

#applying PCA
Xpca = np.reshape(X, (-1, X.shape[2]))
pca = PCA(n_components=30, whiten=True)
Xpca = pca.fit_transform(Xpca)
Xpca = np.reshape(Xpca, (X.shape[0],X.shape[1], 30))

plt.plot( np.cumsum(pca.explained_variance_ratio_))
scale_factor = 1.1
xmin, xmax = plt.xlim()
plt.xlim(xmin * scale_factor, xmax * scale_factor)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance')

def PWZ(X,margin=2):     #padding with zeros
    newX=np.zeros((X.shape[0]+ 2 * margin, X.shape[1] + 2 * margin, X.shape[2]))
    X_offset=margin
    y_offset=margin
    newX[X_offset:X.shape[0] + X_offset , y_offset:X.shape[1] + y_offset:]=X
    return newX

def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):
    margin = int((windowSize) / 2)
    zeroPaddedX = PWZ(X, margin=margin)
    print(zeroPaddedX.shape)
    # split patches
    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))
    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
    patchIndex = 0
    for r in range(margin, zeroPaddedX.shape[0] - margin):
        for c in range(margin, zeroPaddedX.shape[1] - margin):
            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]
            patchesData[patchIndex, :, :, :] = patch
            patchesLabels[patchIndex] = y[r-margin, c-margin]
            patchIndex = patchIndex + 1
    if removeZeroLabels:
        patchesData = patchesData[patchesLabels>0,:,:,:]
        patchesLabels = patchesLabels[patchesLabels>0]
        patchesLabels -= 1
    return patchesData, patchesLabels

"""#Hyperspectral Image Classification using Convolutional Neural Networks
Shambulinga M1, G. Sadashivappa2

https://thesai.org/Downloads/Volume12No6/Paper_82-Hyperspectral_Image_Classification.pdf
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.utils import np_utils
from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, Activation, GlobalMaxPool1D
from keras.layers import Dropout, Input, InputLayer, GlobalMaxPool2D, Conv1D, MaxPool1D, MaxPool2D
from keras.models import Model
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score

from __future__ import print_function


import os
from operator import truediv
from plotly.offline import init_notebook_mode
from tqdm import tqdm

from scipy.io import loadmat
## Read the data.
X = loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']
y = loadmat('Indian_pines_gt.mat')['indian_pines_gt']

#Plotting ground truth
plt.figure(figsize=(8,6))
plt.imshow(y, cmap='nipy_spectral')
plt.axis('off')
plt.show()

#applying PCA
Xpca = np.reshape(X, (-1, X.shape[2]))
pca = PCA(n_components=30, whiten=True)
Xpca = pca.fit_transform(Xpca)
Xpca = np.reshape(Xpca, (X.shape[0],X.shape[1], 30))

plt.plot( np.cumsum(pca.explained_variance_ratio_))
scale_factor = 1.1
xmin, xmax = plt.xlim()
plt.xlim(xmin * scale_factor, xmax * scale_factor)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance')

def PWZ(X,margin=2):     #padding with zeros
    newX=np.zeros((X.shape[0]+ 2 * margin, X.shape[1] + 2 * margin, X.shape[2]))
    X_offset=margin
    y_offset=margin
    newX[X_offset:X.shape[0] + X_offset , y_offset:X.shape[1] + y_offset:]=X
    return newX

def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):
    margin = int((windowSize) / 2)
    zeroPaddedX = PWZ(X, margin=margin)
    print(zeroPaddedX.shape)
    # split patches
    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))
    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
    patchIndex = 0
    for r in range(margin, zeroPaddedX.shape[0] - margin):
        for c in range(margin, zeroPaddedX.shape[1] - margin):
            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]
            patchesData[patchIndex, :, :, :] = patch
            patchesLabels[patchIndex] = y[r-margin, c-margin]
            patchIndex = patchIndex + 1
    if removeZeroLabels:
        patchesData = patchesData[patchesLabels>0,:,:,:]
        patchesLabels = patchesLabels[patchesLabels>0]
        patchesLabels -= 1
    return patchesData, patchesLabels

Xcube, ycube = createImageCubes(Xpca, y, windowSize=13, removeZeroLabels = True)

#Splitting training and testing data
X_train, X_test, y_train, y_test = train_test_split(Xcube, ycube, test_size=0.8, random_state=345)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train = X_train.reshape(-1, 13, 13, 30 )
X_test = X_test.reshape(-1, 13, 13, 30 )

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

model=tf.keras.models.Sequential([

  tf.keras.layers.InputLayer((13, 13, 30)),
  tf.keras.layers.Conv2D(32,kernel_size=(3,3),activation='relu'),
  tf.keras.layers.Conv2D(64,kernel_size=(3,3),activation='relu'),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units=256, activation='relu'),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(units=16, activation='softmax')

])

model.summary()

model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])

fittedModel = model.fit(x=X_train, y=y_train, validation_data=(X_test,y_test), batch_size=256, epochs=80, verbose=2)
#,MyCallback([X_test, y_test])

model.save('HIC_using_CNN_13WS.h5')

from keras.models import load_model

# returns a compiled model
# identical to the previous one
model = load_model('HIC_using_CNN_13WS.h5')

# summarize history for accuracy
plt.plot(fittedModel.history['accuracy'])
plt.plot(fittedModel.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

X123 = PWZ(Xpca,12)

PATCH_SIZE=13
def Patch(data,height_index,width_index):
    height_slice = slice(height_index, height_index+PATCH_SIZE)
    width_slice = slice(width_index, width_index+PATCH_SIZE)
    patch = data[height_slice, width_slice, :]

    return patch

def AA_andEachClassAccuracy(confusion_matrix):
    counter = confusion_matrix.shape[0]
    list_diag = np.diag(confusion_matrix)
    list_raw_sum = np.sum(confusion_matrix, axis=1)
    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
    average_acc = np.mean(each_acc)
    return each_acc, average_acc

height=145
width=145
outputs = np.zeros((height,width))
for i in tqdm(range(height)):
    for j in range(width):
        target = int(y[i,j])
        if target == 0 :
            continue
        else :
            image_patch=Patch(X123,i,j)
            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')
            prediction = (model.predict(X_test_image))
            prediction = np.argmax(prediction, axis=1)
            outputs[i][j] = prediction+1

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

target_names = ['No Class','Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
classification = classification_report((y.astype(int)).flatten(), (outputs.astype(int)).flatten(), target_names=target_names, digits=4)
print(classification)
print('Accuracy Score: ',accuracy_score((y.astype(int)).flatten(), (outputs.astype(int)).flatten()))

confusion= confusion_matrix(y.astype(int).flatten(), outputs.astype(int).flatten())
each_acc, aa = AA_andEachClassAccuracy(confusion)
print('The Average Accuracy is ', aa)
print('The Kappa Coefficient is ', cohen_kappa_score(y.astype(int).flatten(), outputs.astype(int).flatten()))

import seaborn as sn

df_cm = pd.DataFrame(confusion, index = [i for i in target_names], columns = [i for i in target_names])
plt.figure(figsize = (20,14))
sn.heatmap(df_cm, annot=True)

def fgsm_attack(model, image, epsilon):
    image_tensor = tf.convert_to_tensor(image)
    image_tensor = tf.expand_dims(image_tensor, 0)  # Add batch dimension

    with tf.GradientTape() as tape:
        tape.watch(image_tensor)
        prediction = model(image_tensor)

    gradient = tape.gradient(prediction, image_tensor)
    perturbation = epsilon * tf.sign(gradient)
    adversarial_image = image_tensor + perturbation

    return adversarial_image.numpy()

image=Patch(X123,i,j)
img = image.reshape(1,image.shape[0],image.shape[1], image.shape[2], 1).astype('float32')
prediction = (model.predict(img))

eplison=0.1

noisy_img=fgsm_attack(model,image,eplison)

df = pd.DataFrame({"Class": target_names, "Accuracy": each_acc})
df

plt.figure(figsize=(50,15))
plt.bar(target_names, each_acc, width=0.7)

fig, ax = plt.subplots(1,2, figsize=(16,10))
ax[0].imshow(outputs.astype(int), cmap='nipy_spectral')
ax[1].imshow(y, cmap='nipy_spectral')
plt.show()

"""# Hyperspectral Image Classification With Deep Learning Models
Xiaofei Yang , Yunming Ye
"""

Xcube, ycube = createImageCubes(X, y, windowSize=7, removeZeroLabels = True)

#Splitting training and testing data
# window sieze = 7
X_train, X_test, y_train, y_test = train_test_split(Xcube, ycube, test_size=0.3, random_state=345)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train = X_train.reshape(-1, 7, 7, 200, 1 )
X_test = X_test.reshape(-1, 7, 7, 200, 1 )

#Splitting training and testing data
# window size = 27
X_train, X_test, y_train, y_test = train_test_split(Xcube, ycube, test_size=0.3, random_state=345)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train = X_train.reshape(-1, 27, 27, 30, 1 )
X_test = X_test.reshape(-1, 27, 27, 30, 1 )

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

ThreeDCNNmodel=tf.keras.models.Sequential([

  tf.keras.layers.InputLayer((7, 7, 200, 1)),
  tf.keras.layers.Conv3D(16,kernel_size=(3,3,3),strides=(1,1,1),activation='relu'), #23
  tf.keras.layers.Conv3D(32,kernel_size=(3,3,3),strides=(1,1,2),activation='relu'),#11
  tf.keras.layers.Conv3D(64,kernel_size=(3,3,3),strides=(1,1,1),activation='relu'),#9
  tf.keras.layers.Conv3D(128,kernel_size=(1,1,3),strides=(1,1,2),activation='relu'),#7
  tf.keras.layers.Conv3D(256,kernel_size=(1,1,3),strides=(1,1,1),activation='relu'),#5
  tf.keras.layers.Conv3D(512,kernel_size=(1,1,3),strides=(1,1,2),activation='relu'),#3
  tf.keras.layers.Conv3D(1024,kernel_size=(1,1,3),strides=(1,1,1),activation='relu'),
  #tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units=1024, activation='relu'),
  tf.keras.layers.Dense(units=160, activation='relu'),
  #tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(units=16, activation='softmax')

])

ThreeDCNNmodel=tf.keras.models.Sequential([

  tf.keras.layers.InputLayer((27, 27, 30, 1)),
  tf.keras.layers.Conv3D(16,kernel_size=(3,3,3),strides=(1,1,1),activation='relu'), #23
  tf.keras.layers.Conv3D(32,kernel_size=(3,3,3),strides=(1,1,2),activation='relu'),#11
  tf.keras.layers.Conv3D(64,kernel_size=(3,3,3),strides=(1,1,1),activation='relu'),#9
  tf.keras.layers.Conv3D(128,kernel_size=(1,1,3),strides=(1,1,2),activation='relu'),#7
  tf.keras.layers.Conv3D(256,kernel_size=(1,1,3),strides=(1,1,1),activation='relu'),#5
  tf.keras.layers.Conv3D(512,kernel_size=(1,1,3),strides=(1,1,2),activation='relu'),#3
  tf.keras.layers.Conv3D(1024,kernel_size=(1,1,3),strides=(1,1,1),activation='relu'),
  #tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units=1024, activation='relu'),
  tf.keras.layers.Dense(units=160, activation='relu'),
  #tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(units=16, activation='softmax')

])

ThreeDCNNmodel.summary()

ThreeDCNNmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,mode='auto')

fitted3DCNNmodel = ThreeDCNNmodel.fit(x=X_train, y=y_train, validation_data=(X_test,y_test),
                                      batch_size=32, epochs=80, verbose=1) #callbacks=[callback])
#,MyCallback([X_test, y_test])

while True:
  pass

history = fitted3DCNNmodel
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

X123 = PWZ(X,3)

PATCH_SIZE=7
def Patch(data,height_index,width_index):
    height_slice = slice(height_index, height_index+PATCH_SIZE)
    width_slice = slice(width_index, width_index+PATCH_SIZE)
    patch = data[height_slice, width_slice, :]

    return patch

def AA_andEachClassAccuracy(confusion_matrix):
    counter = confusion_matrix.shape[0]
    list_diag = np.diag(confusion_matrix)
    list_raw_sum = np.sum(confusion_matrix, axis=1)
    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
    average_acc = np.mean(each_acc)
    return each_acc, average_acc



height=145
width=145
outputs = np.zeros((height,width))
for i in tqdm(range(height)):
    for j in range(width):
        target = int(y[i,j])
        if target == 0 :
            continue
        else :
            image_patch=Patch(X123,i,j)
            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')
            prediction = (ThreeDCNNmodel.predict(X_test_image))
            prediction = np.argmax(prediction, axis=1)
            outputs[i][j] = prediction+1

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

target_names = ['No Class','Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
classification = classification_report((y.astype(int)).flatten(), (outputs.astype(int)).flatten(), target_names=target_names, digits=4)
print(classification)
print('Accuracy Score: ',accuracy_score((y.astype(int)).flatten(), (outputs.astype(int)).flatten()))

confusion= confusion_matrix(y.astype(int).flatten(), outputs.astype(int).flatten())
each_acc, aa = AA_andEachClassAccuracy(confusion)
print('The Average Accuracy is ', aa)
print('The Kappa Coefficient is ', cohen_kappa_score(y.astype(int).flatten(), outputs.astype(int).flatten()))

import seaborn as sn

df_cm = pd.DataFrame(confusion, index = [i for i in target_names], columns = [i for i in target_names])
plt.figure(figsize = (20,14))
sn.heatmap(df_cm, annot=True)

fig, ax = plt.subplots(1,2, figsize=(16,10))
ax[0].imshow(outputs.astype(int), cmap='nipy_spectral')
ax[1].imshow(y, cmap='nipy_spectral')
plt.show()

ThreeDCNNmodel.save("3D_CNN_7_WS_200bands.h5")

"""# MULTI-DIMENSION CNN FOR HYPERSPECTRAL IMAGE CLASSIFICATON
Haojie Cai

"""

#Splitting training and testing data
Xcube, ycube = createImageCubes(Xpca, y, windowSize=11, removeZeroLabels = True)
X_train, X_test, y_train, y_test = train_test_split(Xcube, ycube, test_size=0.5, random_state=300)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train = X_train.reshape(-1, 11, 11, 30)
X_test = X_test.reshape(-1, 11, 11, 30)

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

from keras.backend import concatenate
from keras.layers.pooling import MaxPool2D
input_layer = Input(shape=(11,11,30))

conv2d_1 = Conv2D(128,(3,3))(input_layer)
batchNorm_1 = BatchNormalization()(conv2d_1)
relu_1 = Activation('relu')(batchNorm_1)

conv2d_2 = Conv2D(256,(3,3))(relu_1)
relu_2 = Activation('relu')(conv2d_2)
maxPool_2 = MaxPool2D(pool_size=(2,2),strides=(2,2))(relu_2)
print(maxPool_2.shape)

global_maxPool_1 = GlobalMaxPool2D()(maxPool_2)
print(global_maxPool_1.shape)

reshaped_gloabl_maxPool_1 = Reshape((8,32))(global_maxPool_1)
conv1d_3 = Conv1D(128,kernel_size=3)(reshaped_gloabl_maxPool_1)
print(conv1d_3.shape)
batchNorm_3 = BatchNormalization()(conv1d_3)
relu_3 = Activation('relu')(batchNorm_3)


conv1d_4 = Conv1D(256,kernel_size=3)(relu_3)
relu_4 = Activation('relu')(conv1d_4)
maxPool_4 = MaxPool1D(pool_size=2,strides=2)(relu_4)

global_maxPool_2 = GlobalMaxPool1D()(relu_4)
print(global_maxPool_2.shape)

feature_connection_module = concatenate([global_maxPool_1,global_maxPool_2])

fully_connected = Dense(256,activation='relu')(feature_connection_module)

output_layer = Dense(16,activation='softmax')(fully_connected)

model = Model(inputs=input_layer, outputs=output_layer)

model.summary()

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='MD_CNN_model_plot.png', show_shapes=True, show_layer_names=True)

model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,mode='auto')



fittedmodel = model.fit(x=X_train, y=y_train, validation_data=(X_test,y_test),
                                      batch_size=32, epochs=80, verbose=1, callbacks=[callback])
#,MyCallback([X_test, y_test])

model.save("MD_CNN2.h5")

model = keras.models.load_model("MD_CNN2.h5")

history = fittedmodel
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



X123 = PWZ(Xpca,12)

PATCH_SIZE=25
def Patch(data,height_index,width_index):
    height_slice = slice(height_index, height_index+PATCH_SIZE)
    width_slice = slice(width_index, width_index+PATCH_SIZE)
    patch = data[height_slice, width_slice, :]

    return patch

def AA_andEachClassAccuracy(confusion_matrix):
    counter = confusion_matrix.shape[0]
    list_diag = np.diag(confusion_matrix)
    list_raw_sum = np.sum(confusion_matrix, axis=1)
    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
    average_acc = np.mean(each_acc)
    return each_acc, average_acc

height=145
width=145
outputs = np.zeros((height,width))
for i in tqdm(range(height)):
    for j in range(width):
        target = int(y[i,j])
        if target == 0 :
            continue
        else :
            image_patch=Patch(X123,i,j)
            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')
            prediction = (model.predict(X_test_image))
            prediction = np.argmax(prediction, axis=1)
            outputs[i][j] = prediction+1

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

target_names = ['No Class','Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
classification = classification_report((y.astype(int)).flatten(), (outputs.astype(int)).flatten(), target_names=target_names, digits=4)
print(classification)
print('Accuracy Score: ',accuracy_score((y.astype(int)).flatten(), (outputs.astype(int)).flatten()))

confusion= confusion_matrix(y.astype(int).flatten(), outputs.astype(int).flatten())
each_acc, aa = AA_andEachClassAccuracy(confusion)
print('The Average Accuracy is ', aa)
print('The Kappa Coefficient is ', cohen_kappa_score(y.astype(int).flatten(), outputs.astype(int).flatten()))

import seaborn as sn

df_cm = pd.DataFrame(confusion, index = [i for i in target_names], columns = [i for i in target_names])
plt.figure(figsize = (20,14))
sn.heatmap(df_cm, annot=True)

fig, ax = plt.subplots(1,2, figsize=(16,10))
ax[0].imshow(outputs.astype(int), cmap='nipy_spectral')
ax[1].imshow(y, cmap='nipy_spectral')
plt.show()



"""# Diverse Region-Based CNN for Hyperspectral Image Classification
Mengmeng Zhang
"""

Xcube, ycube = createImageCubes(Xpca, y, windowSize=11, removeZeroLabels = True)

zeroPaddedX = PWZ(Xpca,5)

zeroPaddedX.shape

def createDiverseRegions(X, y, windowSize=11, removeZeroLabels = True):
    margin = int((windowSize) / 2)
    zeroPaddedX = PWZ(X, margin=margin)
    print(zeroPaddedX.shape)
    # split patches.....Define diverse regions
    patchesGlobal = np.zeros((X.shape[0] * X.shape[1], 11,11, X.shape[2]))
    patchesCentral = np.zeros((X.shape[0] * X.shape[1], 3, 3, X.shape[2]))
    patchesLeft = np.zeros((X.shape[0] * X.shape[1], 11, 7, X.shape[2]))
    patchesRight = np.zeros((X.shape[0] * X.shape[1], 11, 7, X.shape[2]))
    patchesTop = np.zeros((X.shape[0] * X.shape[1], 7, 11, X.shape[2]))
    patchesBottom = np.zeros((X.shape[0] * X.shape[1], 7, 11, X.shape[2]))
    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
    #patchesLabels = []
    #patchesList = []
    patchIndex = 0
    for r in range(margin, zeroPaddedX.shape[0] - margin):
        for c in range(margin, zeroPaddedX.shape[1] - margin):

            if y[r-margin,c-margin] > 0 :
              patchGlobal = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]
              patchesGlobal[patchIndex, :, :, :] = patchGlobal

              patchCentral = zeroPaddedX[r-1:r+1 + 1, c-1:c+1 +1]
              patchesCentral[patchIndex, :, :, :] = patchCentral

              patchLeft = zeroPaddedX[r - margin:r + margin + 1, c- margin:c+1 +1]
              patchesLeft[patchIndex, :, :, :] = patchLeft

              patchRight = zeroPaddedX[r - margin:r + margin + 1, c-1:c+margin +1]
              patchesRight[patchIndex, :, :, :] = patchRight

              patchTop = zeroPaddedX[r-margin:r+1 + 1, c - margin:c + margin + 1]
              patchesTop[patchIndex, :, :, :] = patchTop

              patchBottom = zeroPaddedX[r-1:r+margin + 1, c - margin:c + margin + 1]
              patchesBottom[patchIndex, :, :, :] = patchBottom

              patchesLabels[patchIndex] = y[r-margin, c-margin]


              #patchesLabels.append(y[r-margin, c-margin]-1)
              patchIndex = patchIndex + 1




    if removeZeroLabels:
        patchesGlobal = patchesGlobal[patchesLabels>0, :, :, :]
        patchesCentral = patchesCentral[patchesLabels>0, :, :, :]
        patchesLeft = patchesLeft[patchesLabels>0, :, :, :]
        patchesRight = patchesRight[patchesLabels>0, :, :, :]
        patchesTop = patchesTop[patchesLabels>0, :, :, :]
        patchesBottom = patchesBottom[patchesLabels>0, :, :, :]

        #patchesData = patchesData[patchesLabels>0,:,:,:]
        patchesLabels = patchesLabels[patchesLabels>0]
        patchesLabels -= 1

    return patchesGlobal, patchesCentral, patchesLeft, patchesRight, patchesTop, patchesBottom,\
     np.array(patchesLabels)
    #patchesList = [patchesGlobal, patchesCentral, patchesLeft, patchesRight, patchesTop, patchesBottom]

patchesGlobal, patchesCentral, patchesLeft, patchesRight, patchesTop, patchesBottom,\
 patchesLabels = createDiverseRegions(Xpca,y,11,True)

len(patchesList)

patchesLabels.shape

np.array(patchesList)

#Splitting training and testing data
patchesGlobal_train,patchesGlobal_test, patchesCentral_train,patchesCentral_test, patchesLeft_train,patchesLeft_test, patchesRight_train,patchesRight_test, patchesTop_train,patchesTop_test, patchesBottom_train, patchesBottom_test,\
 patchesLabels_train, patchesLabels_test = train_test_split(patchesGlobal, patchesCentral,\
                                                    patchesLeft, patchesRight, patchesTop, patchesBottom,\
                                                    patchesLabels, test_size=0.2, random_state=300)

patchesLabels_train = np_utils.to_categorical(patchesLabels_train)
patchesLabels_test = np_utils.to_categorical(patchesLabels_test)

from tqdm import tqdm
import pandas as pd
def extract_pixels(dataset, ground_truth):
    df = pd.DataFrame()
    for i in tqdm(range(dataset.shape[2])):
        df = pd.concat([df, pd.DataFrame(dataset[:, :, i].ravel())], axis=1)
    df = pd.concat([df, pd.DataFrame(ground_truth.ravel())], axis=1)
    df.columns = [f'band-{i}' for i in range(1, 1+dataset.shape[2])]+['class']
    return df

df = extract_pixels(X, y)

X_df = df.iloc[:, :-1].values

y_df = df.iloc[:, -1].values

#Splitting training and testing pixels
X_trainPixels, X_testPixels, y_trainPixels, y_testPixels,indices_train, indices_test =\
 train_test_split(X_df, y_df,range(X_df.shape[0]), test_size=0.2, random_state=345)

class CustomDataGen(tf.keras.utils.Sequence):

  def __init__(self,X,y,X_df,y_df,batch_size,training_list,windowSize):
    self.X = X
    self.y = y
    self.X_df = X_df
    self.y_df = y_df
    self.batch_size = batch_size
    self.training_list = training_list
    self.windowSize = windowSize
    self.zeroPaddedX = PWZ(X,int((windowSize) / 2))


  def PWZ(X,margin=2):     #padding with zeros
    newX=np.zeros((X.shape[0]+ 2 * margin, X.shape[1] + 2 * margin, X.shape[2]))
    X_offset=margin
    y_offset=margin
    newX[X_offset:X.shape[0] + X_offset , y_offset:X.shape[1] + y_offset:]=X
    return newX

  def getDiverseRegions(self,windowSize,index):
    margin = int((windowSize) / 2)
    '''row    = int(index / (self.X.shape[0]))
    col = int(index % (self.X.shape[1]))'''

    r    = int(index / (self.X.shape[0]))
    c = int(index % (self.X.shape[1]))

    patchesGlobal = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))
    patchesCentral = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))
    patchesLeft = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))
    patchesRight = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))
    patchesTop = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))
    patchesBottom = np.zeros((self.X.shape[0] * self.X.shape[1], windowSize, windowSize, self.X.shape[2]))



    '''patchGlobal = zeroPaddedX[row : row + 2*margin + 1, col : col + 2*margin + 1]

    patchCentral = zeroPaddedX[row + margin -1 : row + margin +1 + 1, col + margin - 1 : col + margin + +1 +1]

    patchLeft = zeroPaddedX[row : row + 2*margin + 1, col : col + margin +1 +1]

    patchRight = zeroPaddedX[row :row + 2*margin + 1, col + margin -1:col + 2*margin +1]

    patchTop = zeroPaddedX[row:row + margin +1 + 1, col :col + 2*margin + 1]

    patchBottom = zeroPaddedX[row + margin -1:row+ 2*margin + 1, col :col + 2*margin + 1]'''



    patchGlobal = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]

    patchCentral = zeroPaddedX[r-1:r+1 + 1, c-1:c+1 +1]

    patchLeft = zeroPaddedX[r - margin:r + margin + 1, c- margin:c+1 +1]

    patchRight = zeroPaddedX[r - margin:r + margin + 1, c-1:c+margin +1]

    patchTop = zeroPaddedX[r-margin:r+1 + 1, c - margin:c + margin + 1]

    patchBottom = zeroPaddedX[r-1:r+margin + 1, c - margin:c + margin + 1]



    patchLabel = self.y[r,c]

    return patchGlobal, patchCentral, patchLeft, patchRight, patchTop, patchBottom, patchLabel-1



  def __getitem__(self,index):

    input_data = []
    target_data = []

    for idx in range(index*(self.batch_size),(index+1)*(self.batch_size)):
      if idx>=21025 or self.y_df[self.training_list[idx]] == 0:
        continue
      patchGlobal, patchCentral, patchLeft, patchRight, patchTop, patchBottom, patchLabel = \
                                                self.getDiverseRegions(self.windowSize,self.training_list[idx])
      patchGlobal = np.asarray(patchGlobal).astype('float32')
      patchCentral = np.asarray(patchCentral).astype('float32')
      patchLeft = np.asarray(patchLeft).astype('float32')
      patchRight = np.asarray(patchRight).astype('float32')
      patchTop = np.asarray(patchTop).astype('float32')
      patchBottom = np.asarray(patchBottom).astype('float32')
      input_data.extend([[patchGlobal, patchCentral, patchLeft, patchRight, patchTop, patchBottom]])
      print(len(input_data))
      #x_train = np.array([np.array(val) for val in input_data])
      #x_train = np.array([patchGlobal,patchCentral,patchLeft,patchRight,patchTop,patchBottom],dtype='float32')
      target_data.append(np_utils.to_categorical(patchLabel,num_classes=16))

    inputs = np.array(input_data)
    input_data = []
    targets = np.array(target_data)
    np.array(input_data)
    print(inputs.shape)
    print(targets.shape)
    return (inputs,targets)



  #def on_epoch_end(self):


  def __len__(self):
    return int(np.floor(len(self.training_list) / self.batch_size))

train_generator = CustomDataGen(Xpca,y,X_df,y_df,32,indices_train,11)
valid_generator = CustomDataGen(Xpca,y,X_df,y_df,32,indices_test,11)

globalPatch = Input((11,11,30))
centralPatch = Input((3,3,30))
leftPatch = Input((11,7,30))
rightPatch = Input((11,7,30))
topPatch = Input((7,11,30))
bottomPatch = Input((7,11,30))

#Global Patch
globalConv2d_1 = Conv2D(128,(3,3),padding='same')(globalPatch)
globalBatchNorm_1 = BatchNormalization()(globalConv2d_1)
globalRelu_1 = Activation('relu')(globalBatchNorm_1)
globalConv2d_2 = Conv2D(128,(3,3),padding='same')(globalRelu_1)
globalRelu_2 = Activation('relu')(globalConv2d_2)
globalConv2d_3 = Conv2D(128,(1,1),padding='same')(globalRelu_2)
globalRelu_3 = Activation('relu')(globalConv2d_3)
globalConv2d_4 = Conv2D(64,(5,5),padding='same')(globalPatch) #skip connection
globalConv2d_5 = Conv2D(64,(3,3),padding='same')(globalRelu_1) #skip connection
globalConcat = concatenate([globalConv2d_4,globalConv2d_5,globalRelu_3])
globalFlatten = Flatten()(globalConcat)

#Right Patch
rightConv2d_1 = Conv2D(128,(3,3),padding='same')(rightPatch)
rightBatchNorm_1 = BatchNormalization()(rightConv2d_1)
rightRelu_1 = Activation('relu')(rightBatchNorm_1)
rightConv2d_2 = Conv2D(128,(3,3),padding='same')(rightRelu_1)
rightRelu_2 = Activation('relu')(rightConv2d_2)
rightConv2d_3 = Conv2D(128,(1,1),padding='same')(rightRelu_2)
rightRelu_3 = Activation('relu')(rightConv2d_3)
rightConv2d_4 = Conv2D(64,(5,5),padding='same')(rightPatch) #skip connection
rightConv2d_5 = Conv2D(64,(3,3),padding='same')(rightRelu_1) #skip connection
rightConcat = concatenate([rightConv2d_4,rightConv2d_5,rightRelu_3])
rightFlatten = Flatten()(rightConcat)

#Left Patch
leftConv2d_1 = Conv2D(128,(3,3),padding='same')(leftPatch)
leftBatchNorm_1 = BatchNormalization()(leftConv2d_1)
leftRelu_1 = Activation('relu')(leftBatchNorm_1)
leftConv2d_2 = Conv2D(128,(3,3),padding='same')(leftRelu_1)
leftRelu_2 = Activation('relu')(leftConv2d_2)
leftConv2d_3 = Conv2D(128,(1,1),padding='same')(leftRelu_2)
leftRelu_3 = Activation('relu')(leftConv2d_3)
leftConv2d_4 = Conv2D(64,(5,5),padding='same')(leftPatch) #skip connection
leftConv2d_5 = Conv2D(64,(3,3),padding='same')(leftRelu_1) #skip connection
leftConcat = concatenate([leftConv2d_4,leftConv2d_5,leftRelu_3])
leftFlatten = Flatten()(leftConcat)

#Top patch
topConv2d_1 = Conv2D(128,(3,3),padding='same')(topPatch)
topBatchNorm_1 = BatchNormalization()(topConv2d_1)
topRelu_1 = Activation('relu')(topBatchNorm_1)
topConv2d_2 = Conv2D(128,(3,3),padding='same')(topRelu_1)
topRelu_2 = Activation('relu')(topConv2d_2)
topConv2d_3 = Conv2D(128,(1,1),padding='same')(topRelu_2)
topRelu_3 = Activation('relu')(topConv2d_3)
topConv2d_4 = Conv2D(64,(5,5),padding='same')(topPatch) #skip connection
topConv2d_5 = Conv2D(64,(3,3),padding='same')(topRelu_1) #skip connection
topConcat = concatenate([topConv2d_4,topConv2d_5,topRelu_3])
topFlatten = Flatten()(topConcat)

#Bottom Patch
bottomConv2d_1 = Conv2D(128,(3,3),padding='same')(bottomPatch)
bottomBatchNorm_1 = BatchNormalization()(bottomConv2d_1)
bottomRelu_1 = Activation('relu')(bottomBatchNorm_1)
bottomConv2d_2 = Conv2D(128,(3,3),padding='same')(bottomRelu_1)
bottomRelu_2 = Activation('relu')(bottomConv2d_2)
bottomConv2d_3 = Conv2D(128,(1,1),padding='same')(bottomRelu_2)
bottomRelu_3 = Activation('relu')(bottomConv2d_3)
bottomConv2d_4 = Conv2D(64,(5,5),padding='same')(bottomPatch) #skip connection
bottomConv2d_5 = Conv2D(64,(3,3),padding='same')(bottomRelu_1) #skip connection
bottomConcat = concatenate([bottomConv2d_4,bottomConv2d_5,bottomRelu_3])
bottomFlatten = Flatten()(bottomConcat)

#Central Patch
centralConv2d_1 = Conv2D(128,(3,3),padding='same')(centralPatch)
centralBatchNorm_1 = BatchNormalization()(centralConv2d_1)
centralRelu_1 = Activation('relu')(centralBatchNorm_1)
centralConv2d_2 = Conv2D(128,(1,1),padding='same')(centralRelu_1)
centralRelu_2 = Activation('relu')(centralConv2d_2)
centralFlatten = Flatten()(centralRelu_2)

outputDeepFeatureExtractor = concatenate([globalFlatten,rightFlatten,\
                                          leftFlatten,topFlatten,bottomFlatten,centralFlatten])

#Fully Connected Network
fullConnectDense_1 = Dense(512)(outputDeepFeatureExtractor)
fullConnectBatchNorm_1 = BatchNormalization()(fullConnectDense_1)
fullConnectRelu_1 = Activation('relu')(fullConnectBatchNorm_1)
fullConnectDense_2 = Dense(256)(fullConnectRelu_1)
fullConnectRelu_2 = Activation('relu')(fullConnectDense_2)
fullConnectSoftmax = Dense(16,'softmax')(fullConnectRelu_2)

DRCNN_model = Model(inputs=[globalPatch,centralPatch,leftPatch,rightPatch,topPatch,bottomPatch],\
                    outputs=fullConnectSoftmax)

DRCNN_model.summary()

from keras.utils.vis_utils import plot_model
plot_model(DRCNN_model, to_file='DRCNN_model_plot.png', show_shapes=True, show_layer_names=True)

DRCNN_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,mode='auto')

def getPatchesForTraining(X_data):
  patchesGlobal = []
  patchesCentral = []
  patchesLeft = []
  patchesRight = []
  patchesTop = []
  patchesBottom = []

  for i in range(0,len(X_train)):
    patchesGlobal.append(X_train[i][0])
    patchesCentral.append(X_train[i][1])
    patchesLeft.append(X_train[i][2])
    patchesRight.append(X_train[i][3])
    patchesTop.append(X_train[i][4])
    patchesBottom.append(X_train[i][5])

  return np.array(patchesGlobal), np.array(patchesCentral),np.array(patchesLeft),\
          np.array(patchesRight), np.array(patchesTop), np.array(patchesBottom)

patchesGlobal_test, patchesCentral_test, patchesLeft_test, patchesRight_test, patchesTop_test, patchesBottom_test =\
                            getPatchesForTraining(X_test)
patchesGlobal_train, patchesCentral_train, patchesLeft_train, patchesRight_train, patchesTop_train, patchesBottom_train =\
                            getPatchesForTraining(X_train)

fittedmodel = DRCNN_model.fit(x=[patchesGlobal_train,patchesCentral_train,patchesLeft_train,\
                                 patchesRight_train,patchesTop_train,patchesBottom_train], y=patchesLabels_train,
                              validation_data=([patchesGlobal_test,patchesCentral_test,patchesLeft_test,\
                                 patchesRight_test,patchesTop_test,patchesBottom_test],patchesLabels_test),
                                      batch_size=32, epochs=80, verbose=1, callbacks=[callback])

DRCNN_model.save("DRCNN2.h5")

model = keras.models.load_model("DRCNN2.h5")

history = fittedmodel
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model = keras.models.load_model("DRCNN2.h5")

outputs = model.predict(x=[patchesGlobal_test,patchesCentral_test,patchesLeft_test,\
                                 patchesRight_test,patchesTop_test,patchesBottom_test])

outputs.shape

predictions = outputs.argmax(axis=1).flatten() + 1

predictions.shape

y = patchesLabels_test.argmax(axis=1).flatten() + 1

max(predictions)

max(y)



from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

target_names = ['No Class','Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
classification = classification_report(y, predictions, digits=4)
print(classification)
print('Accuracy Score: ',accuracy_score(y, predictions))

print('The Kappa Coefficient is ', cohen_kappa_score(y, predictions))

confusion= confusion_matrix(y, predictions)



import seaborn as sn
target_names = ['Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
df_cm = pd.DataFrame(confusion, index = [i for i in target_names], columns = [i for i in target_names])
plt.figure(figsize = (20,14))
sn.heatmap(df_cm, annot=True)



X123 = PWZ(Xpca,5)

PATCH_SIZE=25
def Patch(data,height_index,width_index):
    '''height_slice = slice(height_index, height_index+PATCH_SIZE)
    width_slice = slice(width_index, width_index+PATCH_SIZE)
    patch = data[height_slice, width_slice, :]'''

    height_slice_global = slice(height_index, height_index+11)
    width_slice_global = slice(width_index, width_index+11)
    patch_global = data[height_slice_global, width_slice_global, :]

    height_slice_central = slice(height_index+4, height_index+7)
    width_slice_central = slice(width_index+4, width_index+7)
    patch_central = data[height_slice_central, width_slice_central, :]

    height_slice_left = slice(height_index, height_index+11)
    width_slice_left = slice(width_index, width_index+7)
    patch_left = data[height_slice_left, width_slice_left, :]

    height_slice_right = slice(height_index, height_index+11)
    width_slice_right = slice(width_index+4, width_index+11)
    patch_right = data[height_slice_right, width_slice_right, :]

    height_slice_top = slice(height_index, height_index+7)
    width_slice_top = slice(width_index, width_index+11)
    patch_top = data[height_slice_top, width_slice_top, :]

    height_slice_bottom = slice(height_index+4, height_index+11)
    width_slice_bottom = slice(width_index, width_index+11)
    patch_bottom = data[height_slice_bottom, width_slice_bottom, :]



    return patch_global, patch_central, patch_left, patch_right, patch_top, patch_bottom

def AA_andEachClassAccuracy(confusion_matrix):
    counter = confusion_matrix.shape[0]
    list_diag = np.diag(confusion_matrix)
    list_raw_sum = np.sum(confusion_matrix, axis=1)
    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
    average_acc = np.mean(each_acc)
    return each_acc, average_acc

height=145
width=145
outputs = np.zeros((height,width))
for i in tqdm(range(height)):
    for j in range(width):
        target = int(y[i,j])
        if target == 0 :
            continue
        else :
            patch_global, patch_central, patch_left, \
            patch_right, patch_top, patch_bottom=Patch(X123,i,j)

            patch_global = patch_global.reshape(1,patch_global.shape[0],patch_global.shape[1], patch_global.shape[2], 1).astype('float32')
            patch_central = patch_central.reshape(1,patch_central.shape[0],patch_central.shape[1], patch_central.shape[2], 1).astype('float32')
            patch_left = patch_left.reshape(1,patch_left.shape[0],patch_left.shape[1], patch_left.shape[2], 1).astype('float32')
            patch_right = patch_right.reshape(1,patch_right.shape[0],patch_right.shape[1], patch_right.shape[2], 1).astype('float32')
            patch_top = patch_top.reshape(1,patch_top.shape[0],patch_top.shape[1], patch_top.shape[2], 1).astype('float32')
            patch_bottom = patch_bottom.reshape(1,patch_bottom.shape[0],patch_bottom.shape[1], patch_bottom.shape[2], 1).astype('float32')

            prediction = (model.predict(x=[patch_global, patch_central, patch_left, \
                                                patch_right, patch_top, patch_bottom]))
            prediction = np.argmax(prediction, axis=1)
            outputs[i][j] = prediction+1

np.save("DRCNN_outputs.npy",outputs)

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

target_names = ['No Class','Alfalfa',	'Corn-notill', 'Corn-mintill','Corn',	'Grass-pasture','Grass-trees','Grass-pasture-mowed','Hay-windrowed','Oats','Soybean-notill','Soybean-mintill','Soybean-clean', 'Wheat',	'Woods',	'Buildings Grass Trees Drives',	'Stone Steel Towers']
classification = classification_report((y.astype(int)).flatten(), (outputs.astype(int)).flatten(), target_names=target_names, digits=4)
print(classification)
print('Accuracy Score: ',accuracy_score((y.astype(int)).flatten(), (outputs.astype(int)).flatten()))

confusion= confusion_matrix(y.astype(int).flatten(), outputs.astype(int).flatten())
each_acc, aa = AA_andEachClassAccuracy(confusion)
print('The Average Accuracy is ', aa)
print('The Kappa Coefficient is ', cohen_kappa_score(y.astype(int).flatten(), outputs.astype(int).flatten()))

import seaborn as sn

df_cm = pd.DataFrame(confusion, index = [i for i in target_names], columns = [i for i in target_names])
plt.figure(figsize = (20,14))
sn.heatmap(df_cm, annot=True)

fig, ax = plt.subplots(1,2, figsize=(16,10))
ax[0].imshow(outputs.astype(int), cmap='nipy_spectral')
ax[1].imshow(y, cmap='nipy_spectral')
plt.show()

